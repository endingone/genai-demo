{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
      "metadata": {},
      "source": [
        "# Understanding Memory in LLMs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
      "metadata": {},
      "source": [
        "이전 노트북에서는 OpenAI 모델이 Azure AI 검색 쿼리의 결과를 향상시키는 방법을 성공적으로 살펴봤습니다. \n",
        "하지만 LLM과 대화에 참여하는 방법은 아직 발견하지 못했습니다. 예를 들어, [Bing Chat](http://chat.bing.com/)을 사용하면 이전 응답을 이해하고 참조할 수 있기 때문에 이것이 가능합니다.\n",
        "LLM(대규모 언어 모델)에 메모리가 있다는 일반적인 오해가 있습니다. 이는 사실이 아닙니다. 지식을 보유하고 있기는 하지만 이전에 질문했던 정보를 기억하지는 못합니다.\n",
        "이 노트북의 목표는 프롬프트와 문맥을 활용하여 LLM에 효과적으로 \"메모리를 부여\"하는 방법을 설명하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
      "metadata": {
        "gather": {
          "logged": 1713623975213
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "from typing import List\n",
        "\n",
        "from IPython.display import Markdown, HTML, display  \n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "#custom libraries that we will use later in the app\n",
        "from common.utils import CustomAzureSearchRetriever, get_answer\n",
        "from common.prompts import DOCSEARCH_PROMPT\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "import logging\n",
        "\n",
        "# Get the root logger\n",
        "logger = logging.getLogger()\n",
        "# Set the logging level to a higher level to ignore INFO messages\n",
        "logger.setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
      "metadata": {
        "gather": {
          "logged": 1713623975524
        }
      },
      "outputs": [],
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
      "metadata": {},
      "source": [
        "### Let's start with the basics\n",
        "아주 간단한 예제를 사용하여 Azure OpenAI의 GPT 모델에 메모리가 있는지 확인해 보겠습니다. 다시 langchain을 사용하여 코드를 간소화하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
      "metadata": {
        "gather": {
          "logged": 1713623976317
        }
      },
      "outputs": [],
      "source": [
        "QUESTION = \"What is our mission?\"\n",
        "FOLLOW_UP_QUESTION = \"What was my prior question?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
      "metadata": {
        "gather": {
          "logged": 1713623976745
        }
      },
      "outputs": [],
      "source": [
        "COMPLETION_TOKENS = 1000\n",
        "# Create an OpenAI instance\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
        "                      temperature=0.5, max_tokens=COMPLETION_TOKENS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
      "metadata": {
        "gather": {
          "logged": 1713623977029
        }
      },
      "outputs": [],
      "source": [
        "# We create a very simple prompt template, just the question as is:\n",
        "output_parser = StrOutputParser()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant that give thorough responses to users.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
      "metadata": {
        "gather": {
          "logged": 1713623978783
        }
      },
      "outputs": [],
      "source": [
        "# Let's see what the GPT model responds\n",
        "chain = prompt | llm | output_parser\n",
        "response_to_initial_question = chain.invoke({\"input\": QUESTION})\n",
        "display(Markdown(response_to_initial_question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
      "metadata": {
        "gather": {
          "logged": 1713623979473
        }
      },
      "outputs": [],
      "source": [
        "#Now let's ask a follow up question\n",
        "printmd(chain.invoke({\"input\": FOLLOW_UP_QUESTION}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "보시다시피 방금 응답한 내용을 기억하지 못하거나 때로는 시스템 프롬프트에 따라 응답하거나 무작위로 응답하기도 합니다. \n",
        "\n",
        "이는 LLM에 메모리가 없다는 것을 증명하며, 이와 같이 프롬프트의 일부로 대화 기록으로 메모리를 제공해야 한다는 것을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
      "metadata": {
        "gather": {
          "logged": 1713623979764
        }
      },
      "outputs": [],
      "source": [
        "hist_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "    {history}\n",
        "    Human: {question}\n",
        "    AI:\n",
        "\"\"\"\n",
        ")\n",
        "chain = hist_prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
      "metadata": {
        "gather": {
          "logged": 1713623979982
        }
      },
      "outputs": [],
      "source": [
        "Conversation_history = \"\"\"\n",
        "Human: {question}\n",
        "AI: {response}\n",
        "\"\"\".format(question=QUESTION, response=response_to_initial_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
      "metadata": {
        "gather": {
          "logged": 1713623980279
        }
      },
      "outputs": [],
      "source": [
        "printmd(chain.invoke({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
      "metadata": {},
      "source": [
        "**Bingo!**, 이제 LLM을 사용하여 챗봇을 만드는 방법을 알았으므로 대화의 상태/이력을 유지하기 위해 매번 Context로 전달하기만 하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
      "metadata": {},
      "source": [
        "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9787ffb6-2b11-4b03-92fc-9443cd1f2ab9",
      "metadata": {},
      "source": [
        "Langchain 웹사이트에서 발췌:\n",
        "    \n",
        "메모리 시스템은 읽기와 쓰기라는 두 가지 기본 작업을 지원해야 합니다. 모든 체인은 특정 입력을 기대하는 몇 가지 핵심 실행 로직을 정의한다는 점을 기억하세요. 이러한 입력 중 일부는 사용자가 직접 제공하지만, 일부는 메모리에서 제공될 수도 있습니다. 체인은 주어진 실행에서 메모리 시스템과 두 번 상호 작용합니다.\n",
        "\n",
        "    초기 사용자 입력을 받은 후 코어 로직을 실행하기 전에 체인은 메모리 시스템에서 읽고 사용자 입력을 보강합니다.\n",
        "    핵심 로직을 실행한 후 답을 반환하기 전에 체인은 현재 실행의 입력과 출력을 메모리에 기록하여 향후 실행에서 참조할 수 있도록 합니다.\n",
        "    \n",
        "따라서 이 프로세스는 응답에 지연을 추가하지만 필요한 지연입니다 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36e8f14-e566-4ae9-a7d4-6dee7f469dad",
      "metadata": {},
      "source": [
        "![image](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
      "metadata": {
        "gather": {
          "logged": 1713623980559
        }
      },
      "outputs": [],
      "source": [
        "index_name = \"cogsrch-index-hrdocs\"\n",
        "indexes = [index_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b01852c2-6192-496c-adff-4270f9380469",
      "metadata": {
        "gather": {
          "logged": 1713623980785
        }
      },
      "outputs": [],
      "source": [
        "# Initialize our custom retriever \n",
        "retriever = CustomAzureSearchRetriever(indexes=indexes, topK=10, reranker_threshold=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "633937e8-18e6-43f2-b4d5-fc36157a4d97",
      "metadata": {},
      "source": [
        "\n",
        "prompts.py를 자세히 살펴보면 `DOCSEARCH_PROMPT`에 `history`라는 선택적 변수가 있습니다. 이제 이 변수를 사용할 때입니다. 기본적으로 프롬프트에 대화를 삽입하여 LLM이 응답하기 전에 이를 인식할 수 있도록 하는 플레이스 홀더입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035fa6e6-226c-400f-a504-30255385f43b",
      "metadata": {},
      "source": [
        "**Now let's add memory to it:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8c9381-08d0-4808-9ab1-78156ca1be6e",
      "metadata": {
        "gather": {
          "logged": 1713623981027
        }
      },
      "outputs": [],
      "source": [
        "store = {} # Our first memory will be a dictionary in memory\n",
        "\n",
        "# We have to define a custom function that takes a session_id and looks somewhere\n",
        "# (in this case in a dictionary in memory) for the conversation\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ff51e1-2b1e-4c67-965d-1c2e2f55e005",
      "metadata": {
        "gather": {
          "logged": 1713623981264
        }
      },
      "outputs": [],
      "source": [
        "# We use our original chain with the retriever but removing the StrOutputParser\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever, \n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"history\": itemgetter(\"history\")\n",
        "    }\n",
        "    | DOCSEARCH_PROMPT\n",
        "    | llm\n",
        ")\n",
        "\n",
        "## Then we pass the above chain to another chain that adds memory to it\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        ") | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e582915-243f-42cb-bb1e-c35a20ee0b9f",
      "metadata": {
        "gather": {
          "logged": 1713623981503
        }
      },
      "outputs": [],
      "source": [
        "# This is where we configure the session id\n",
        "config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
        "\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff493b1-b133-4880-a040-e80f7460e7af",
      "metadata": {},
      "source": [
        "아래에서 호출에 `history` 변수를 추가하는 것을 확인할 수 있습니다. 이 변수는 프롬프트 내의 채팅 기록을 보관합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d91a7ff4-6148-459d-917c-37302805dd09",
      "metadata": {
        "gather": {
          "logged": 1713623991074
        }
      },
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25dfc233-450f-4671-8f1c-0b446e46f048",
      "metadata": {
        "gather": {
          "logged": 1713623994502
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67073c2-9a82-4e44-a9e2-48fe868c1634",
      "metadata": {
        "gather": {
          "logged": 1713623997488
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": \"Thank you! Good bye\"},config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87405173",
      "metadata": {},
      "source": [
        "## Using CosmosDB as persistent memory\n",
        "\n",
        "이전 셀에서 챗봇에 로컬 RAM 메모리를 추가했습니다. 그러나 이는 영구적인 것이 아니며 앱 사용자의 세션이 종료되면 삭제됩니다. 따라서 분석 및 감사뿐만 아니라 향후에 추천을 제공하려는 경우에도 각 봇 사용자 대화를 영구적으로 저장하기 위해 데이터베이스를 사용해야 합니다. \n",
        "\n",
        "여기서는 향후 감사를 위해 대화 내역을 CosmosDB에 저장하겠습니다.\n",
        "\n",
        "LangChain에서 CosmosDBChatMessageHistory를 사용하는 클래스를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
      "metadata": {
        "gather": {
          "logged": 1713623997734
        }
      },
      "outputs": [],
      "source": [
        "# Create the function to retrieve the conversation\n",
        "\n",
        "def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n",
        "    cosmos = CosmosDBChatMessageHistory(\n",
        "        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
        "        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
        "        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
        "        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
        "        session_id=session_id,\n",
        "        user_id=user_id\n",
        "        )\n",
        "\n",
        "    # prepare the cosmosdb instance\n",
        "    cosmos.prepare_cosmos()\n",
        "    return cosmos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f4179b-c1c7-49da-9c80-a42c275ed4d6",
      "metadata": {
        "gather": {
          "logged": 1713623997961
        }
      },
      "outputs": [],
      "source": [
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"user_id\",\n",
        "            annotation=str,\n",
        "            name=\"User ID\",\n",
        "            description=\"Unique identifier for the user.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"Unique identifier for the conversation.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "    ],\n",
        ") | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf1f1f0-6e46-4136-9f33-4e46617b7d4f",
      "metadata": {
        "gather": {
          "logged": 1713623998197
        }
      },
      "outputs": [],
      "source": [
        "# This is where we configure the session id and user id\n",
        "random_session_id = \"session\"+ str(random.randint(1, 1000))\n",
        "ramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n",
        "\n",
        "config={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}\n",
        "\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3c32f4-f883-4045-91f9-ca317c2d01fe",
      "metadata": {
        "gather": {
          "logged": 1713624011048
        }
      },
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e29643b-a531-4117-8e85-9c88a625cf02",
      "metadata": {
        "gather": {
          "logged": 1713624014588
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50146f05-5ef6-484f-a8ec-9631643054f2",
      "metadata": {
        "gather": {
          "logged": 1713624018535
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"Can you tell me a one line summary of our conversation?\"},\n",
        "    config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc02369-904c-4063-93e1-fff24fe6a3ab",
      "metadata": {
        "gather": {
          "logged": 1713624022515
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"Thank you very much!\"},\n",
        "    config=config))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d60faa-1446-4c07-8970-0f9712c33b2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"I do have one more question, why did you give me a one line summary?\"},\n",
        "    config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe748aa-6116-4a7a-97e6-f1c680dd23ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"why not 2?\"},\n",
        "    config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc5ac98",
      "metadata": {},
      "source": [
        "#### Let's check our Azure CosmosDB to see the whole conversation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
      "metadata": {},
      "source": [
        "![CosmosDB Memory](./images/cosmos-chathistory.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
      "metadata": {},
      "source": [
        "# Summary\n",
        "##### 애플리케이션에 메모리를 추가하면 사용자가 대화를 할 수 있지만, 이 기능은 LLM과 함께 제공되는 것이 아니라 질문의 맥락에 따라 메모리를 LLM에 제공해야 합니다.\n",
        "\n",
        "CosmosDB를 사용해 지속적 메모리를 추가했습니다.\n",
        "\n",
        "현재 사용하고 있는 체인이 똑똑하지만 그다지 많지는 않다는 것을 알 수 있습니다. 메모리를 추가했지만, 입력에 관계없이 매번 비슷한 문서를 검색합니다. 효율적이지 않은 것 같지만, 어쨌든 데이터 봇과의 첫 번째 RAG 토크가 거의 마무리 단계에 있습니다.\n",
        "\n",
        "\n",
        "## <u>Important Note</u>:<br>\n",
        "계속 진행하면서 모든 코드는 GPT-3.5 모델(1106 이상)과 계속 호환되지만, GPT-4로 전환할 것을 적극 권장합니다. 그 이유는 다음과 같습니다.\n",
        "\n",
        "**GPT-3.5-Turbo**는 7세 어린이에 비유할 수 있습니다. 간결한 지시를 내릴 수는 있지만, 때때로 그 지시를 정확하게 따르는 데 어려움을 겪습니다(너무 신뢰할 수 없음). 또한 제한된 '메모리'(토큰 context)로 인해 지속적인 대화가 어려울 수 있습니다. 반응도 깊지 않고 단순합니다.\n",
        "\n",
        "\n",
        "**GT-4-Turbo**는 10-12세 아동의 능력을 보여줍니다. 추론 능력이 향상되고, 지시를 일관되게 따르며, 정답률이 더 높습니다. 지시에 대한 기억 유지력(더 큰 context의 크기)이 확장되어 있으며, 지시를 따르는 능력이 뛰어납니다. 답변이 깊고 철저합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
      "metadata": {},
      "source": [
        "# NEXT\n",
        "이제 챗봇을 구동할 수 있는 스마트 검색 엔진을 만드는 방법을 알게 되었습니다!!! 좋아요!\n",
        "\n",
        "다음 노트북 6에서는 첫 번째 RAG 봇을 만들어 보겠습니다. 이를 위해 에이전트의 개념을 소개하겠습니다."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
